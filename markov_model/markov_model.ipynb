{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_frost_file_path = \"../data/robert_frost_small.txt\"\n",
    "edgar_allan_poe_file_path = \"../data/edgar_allan_poe_small.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is incomplete, still need to do lemmatization and remove puctuation , stopword.\n",
    "def preprocessing_word(lst):\n",
    "    return [x.lower().replace(\"''\", \"\").replace(\"''\", \"\") for x in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_tokenize(file_path):\n",
    "    '''\n",
    "    This function purpose is to read file and convert to list of tokenize\n",
    "    '''\n",
    "    tokenize_list = []\n",
    "    f = open(file_path, \"r\")\n",
    "    for x in f:\n",
    "        tokenize_list.append(x.split())\n",
    "        \n",
    "    # remove empty array\n",
    "    tokenize_list = [ x for x in tokenize_list if len(x) != 0]\n",
    "    \n",
    "    #preprocessing\n",
    "    tmp = []\n",
    "    for x in tokenize_list:\n",
    "        tmp.append(preprocessing_word(x))\n",
    "        \n",
    "    tokenize_list = tmp\n",
    "        \n",
    "    return tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 66)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file and convert to tokenize by function file_to_tokenize\n",
    "robert_frost_tokenize_list = file_to_tokenize(robert_frost_file_path)\n",
    "edgar_allan_poe_tokenize_list = file_to_tokenize(edgar_allan_poe_file_path)\n",
    "len(robert_frost_tokenize_list),len(edgar_allan_poe_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood,'],\n",
       " ['and', 'sorry', 'i', 'could', 'not', 'travel', 'both'],\n",
       " ['and', 'be', 'one', 'traveler,', 'long', 'i', 'stood'],\n",
       " ['and', 'looked', 'down', 'one', 'as', 'far', 'as', 'i', 'could'],\n",
       " ['to', 'where', 'it', 'bent', 'in', 'the', 'undergrowth;']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robert_frost_tokenize_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lo!', 'death', 'hath', \"rear'd\", 'himself', 'a', 'throne'],\n",
       " ['in', 'a', 'strange', 'city,', 'all', 'alone,'],\n",
       " ['far', 'down', 'within', 'the', 'dim', 'west'],\n",
       " ['where',\n",
       "  'the',\n",
       "  'good,',\n",
       "  'and',\n",
       "  'the',\n",
       "  'bad,',\n",
       "  'and',\n",
       "  'the',\n",
       "  'worst,',\n",
       "  'and',\n",
       "  'the',\n",
       "  'best,'],\n",
       " ['have', 'gone', 'to', 'their', 'eternal', 'rest.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgar_allan_poe_tokenize_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 66)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robert_frost_label_list = list(np.zeros(len(robert_frost_tokenize_list)))\n",
    "edgar_allan_poe_label_list = list(np.ones(len(edgar_allan_poe_tokenize_list)))\n",
    "\n",
    "len(robert_frost_label_list),len(edgar_allan_poe_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data_x = robert_frost_tokenize_list\n",
    "#all_data_y= robert_frost_label_list+edgar_allan_poe_label_list\n",
    "\n",
    "# len(all_data_x),len(all_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 9, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(robert_frost_tokenize_list, robert_frost_label_list, test_size=0.2, random_state=42)\n",
    "len(X_train),len(y_train),len(X_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['and', 'looked', 'down', 'one', 'as', 'far', 'as', 'i', 'could'],\n",
       " ['and', 'having', 'perhaps', 'the', 'better', 'claim'],\n",
       " ['my', 'little', 'horse', 'must', 'think', 'it', 'queer'],\n",
       " ['the', 'woods', 'are', 'lovely,', 'dark', 'and', 'deep,'],\n",
       " ['and', 'that', 'has', 'made', 'all', 'the', 'difference.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'hold', 'with', 'those', 'who', 'favor', 'fire.'],\n",
       " ['to', 'stop', 'without', 'a', 'farmhouse', 'near'],\n",
       " ['between', 'the', 'woods', 'and', 'frozen', 'lake'],\n",
       " ['is', 'also', 'great'],\n",
       " ['and', 'miles', 'to', 'go', 'before', 'i', 'sleep.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_unique_index(lst):\n",
    "    unique_word_index_dict = {}\n",
    "    word_index = 0\n",
    "    for data in lst:\n",
    "        tmp = np.asarray(data)\n",
    "        unique_tmp = np.unique(tmp)\n",
    "        for word in unique_tmp:\n",
    "            if not word in unique_word_index_dict:\n",
    "                unique_word_index_dict[word] = word_index\n",
    "                word_index += 1\n",
    "    return unique_word_index_dict,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_word_index_dict,X_train_word_index = mapping_unique_index(X_train)\n",
    "X_test_unique_word_index_dict,X_test_word_index = mapping_unique_index(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0, 'as': 1, 'could': 2, 'down': 3, 'far': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(X_train_unique_word_index_dict.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 158)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a key that found in test set but not in trainset\n",
    "# in case, want to visualize\n",
    "X_train_keys_set= set(list(X_train_unique_word_index_dict.keys()))\n",
    "X_test_keys_set= set(list(X_test_unique_word_index_dict.keys()))\n",
    "\n",
    "not_found_keys = X_test_keys_set - X_train_keys_set\n",
    "\n",
    "# create not found key to have unknow_word_key\n",
    "unknow_word_key = 'unk'\n",
    "X_train_unique_word_index_dict[unknow_word_key] = X_train_word_index\n",
    "\n",
    "# add unknow index\n",
    "X_train_word_index += 1\n",
    "unknow_word_index =X_train_word_index\n",
    "\n",
    "X_train_unique_word_index_dict[unknow_word_key],X_train_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'and', 1: 'as', 2: 'could', 3: 'down', 4: 'far'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dictionary for mapping from index to word \n",
    "X_train_unique_index_word_dict = dict((v,k) for k,v in X_train_unique_word_index_dict.items())\n",
    "dict(list(X_train_unique_index_word_dict.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique word list -> M\n",
    "unique_word_train = list(X_train_unique_index_word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_word_str_to_int(lst,mapping_dict):\n",
    "    word_int_list = []\n",
    "    \n",
    "    for sentence in lst:\n",
    "        tmp = [mapping_dict[word] if word in mapping_dict else mapping_dict[unknow_word_key] for word in sentence]\n",
    "        word_int_list.append(tmp)\n",
    "        \n",
    "    return word_int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from list of string of word to list of index of word\n",
    "X_train_int = covert_word_str_to_int(X_train,X_train_unique_word_index_dict)\n",
    "X_test_int = covert_word_str_to_int(X_test,X_train_unique_word_index_dict)\n",
    "\n",
    "X_train_int = np.asarray(X_train_int)\n",
    "X_test_int = np.asarray(X_test_int)\n",
    "\n",
    "len(X_train_int),len(X_test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 6, 3, 7, 1, 4, 1, 5, 2]), list([0, 10, 11, 12, 8, 9]),\n",
       "       list([17, 15, 13, 16, 19, 14, 18]),\n",
       "       list([12, 24, 20, 23, 21, 0, 22]),\n",
       "       list([0, 29, 27, 28, 25, 12, 26])], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_int[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([5, 157, 53, 157, 157, 157, 157]),\n",
       "       list([41, 157, 157, 30, 157, 157]),\n",
       "       list([157, 12, 24, 0, 157, 157]), list([80, 157, 157]),\n",
       "       list([0, 112, 41, 111, 110, 5, 157])], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_int[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of initial states \n",
    "def create_initial_state(lst,word_number):\n",
    "    initial_states_dict = {}\n",
    "    number_of_sequence = len(lst)\n",
    "    for x in lst:\n",
    "        count_x = 0\n",
    "        for y in lst:\n",
    "            if x[0] == y[0]:\n",
    "                count_x += 1\n",
    "        \n",
    "        #smoothing by add 1\n",
    "        initial_states_dict[x[0]]= (count_x+1)/(number_of_sequence+word_number)\n",
    "        \n",
    "    # unknow word become starting word\n",
    "    initial_states_dict[unknow_word_key]    = (1)/(number_of_sequence+word_number)\n",
    "    return initial_states_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.04639175257731959,\n",
       " 17: 0.010309278350515464,\n",
       " 12: 0.020618556701030927,\n",
       " 35: 0.015463917525773196,\n",
       " 43: 0.010309278350515464}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_initial_states_dict = create_initial_state(X_train_int,X_train_word_index)\n",
    "dict(list(X_train_initial_states_dict.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_consecutive_pair_word(sentences,word_1,word_2,word_number):\n",
    "    count_word_1 = 0\n",
    "    count_word_1_to_word_2 = 0\n",
    "#     print(word_1,word_2,type(word_1),type(word_2))\n",
    "    \n",
    "    for sentence in sentences:\n",
    "#         tmp_arr = sentence\n",
    "#         found_word_1 = [1 for i, x in enumerate(sentence) if x ==word_1]\n",
    "#         found_word_1 = np.isin(tmp_arr, [word_1])\n",
    "#         found_word_2 = np.in1d(tmp_arr, [word_2])\n",
    "#         found_word_1_word_2 = found_word_1 | found_word_2\n",
    "        \n",
    "#         print(tmp_arr)\n",
    "#         print(found_word_1)\n",
    "#         print(found_word_2)\n",
    "#         print(found_word_1_word_2)\n",
    "#         found_pair_list  = [[s, e] for s, e in zip(found_word_1_word_2, found_word_1_word_2[1:]) if (s==True)and (e==True)]\n",
    "#         count_word_1_to_word_2 += len(found_pair_list)\n",
    "#         count_word_1 += sum(found_word_1)\n",
    "        start_w = 0\n",
    "        stop_w = len(sentence)-1\n",
    "        for i,local_word in enumerate(sentence[start_w:stop_w],start_w):\n",
    "            if local_word == word_1 and sentence[i+1] == word_2:\n",
    "                count_word_1_to_word_2 += 1\n",
    "            if local_word == word_1:\n",
    "                count_word_1 += 1\n",
    "    \n",
    "    # smooting by add 1\n",
    "    return (count_word_1_to_word_2+1)/(count_word_1+word_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_transition_matrix(word_unique_list,sentences,word_number):\n",
    "    state_transition_matrix = np.zeros((word_number, word_number))\n",
    "    already_done_compute_word_list = []\n",
    "    for i,word_1 in enumerate(word_unique_list):\n",
    "        for j,word_2 in enumerate(word_unique_list):\n",
    "            key = str(word_1)+\"-\"+str(word_2)\n",
    "            if (key in already_done_compute_word_list ):\n",
    "                continue\n",
    "                \n",
    "            prob_pair = count_consecutive_pair_word(sentences,word_1,word_2,word_number)\n",
    "            state_transition_matrix[i][j] = prob_pair\n",
    "            already_done_compute_word_list.append(key)\n",
    "#         break\n",
    "            \n",
    "    return state_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.4658477306365967 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00584795, 0.00584795, 0.00584795, ..., 0.00584795, 0.00584795,\n",
       "        0.00584795],\n",
       "       [0.00617284, 0.00617284, 0.00617284, ..., 0.00617284, 0.00617284,\n",
       "        0.00617284],\n",
       "       [0.00628931, 0.00628931, 0.00628931, ..., 0.00628931, 0.00628931,\n",
       "        0.00628931],\n",
       "       ...,\n",
       "       [0.00628931, 0.00628931, 0.00628931, ..., 0.00628931, 0.00628931,\n",
       "        0.00628931],\n",
       "       [0.00628931, 0.00628931, 0.00628931, ..., 0.00628931, 0.00628931,\n",
       "        0.00628931],\n",
       "       [0.00632911, 0.00632911, 0.00632911, ..., 0.00632911, 0.00632911,\n",
       "        0.00632911]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "state_transition_matrix = create_state_transition_matrix(unique_word_train,X_train_int,X_train_word_index)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "state_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 158)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_transition_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_sentences,initial_states_dict,state_transition_matrix):\n",
    "    \n",
    "    prob =[]\n",
    "    word_max_index = state_transition_matrix.shape[1]\n",
    "    for sentence in test_sentences:\n",
    "        initial_start_prob = 0\n",
    "        if sentence[0] in initial_states_dict:\n",
    "            initial_start_prob = initial_states_dict[sentence[0]]\n",
    "        else:\n",
    "            initial_start_prob = initial_states_dict[unknow_word_key]\n",
    "            \n",
    "        # stretch value by log function\n",
    "        initial_start_prob =math.log(initial_start_prob)\n",
    "        \n",
    "        sum_of_prob = 0\n",
    "        \n",
    "        start_w = 1\n",
    "        stop_w = len(sentence)-1\n",
    "        \n",
    "        for i,local_word in enumerate(sentence[start_w:stop_w],start_w):\n",
    "            if local_word < 0 or local_word > word_max_index or  sentence[i+1] < 0 or sentence[i+1] > word_max_index:\n",
    "                sum_of_prob = -100\n",
    "                break\n",
    "            sum_of_prob += math.log(state_transition_matrix[local_word][sentence[i+1]])\n",
    "        \n",
    "        local_prob = initial_start_prob+sum_of_prob\n",
    "        prob.append(local_prob)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-33.406346566732324,\n",
       " -20.640555903198283,\n",
       " -26.47226586258264,\n",
       " -25.851878036305123,\n",
       " -25.0288130876228,\n",
       " -29.342345614958187,\n",
       " -30.866294380145447,\n",
       " -21.409592210113047,\n",
       " -30.167337286944495,\n",
       " -26.520390321634842,\n",
       " -17.787280623793862,\n",
       " -12.96422230208399,\n",
       " -22.150498419426956,\n",
       " -21.463986282178844,\n",
       " -24.89382923901547,\n",
       " -21.45148611941461,\n",
       " -25.791618844786925,\n",
       " -30.534336114224867,\n",
       " -26.465996249569045,\n",
       " -21.770309741419553,\n",
       " -24.61121770763681,\n",
       " -26.122572738669692,\n",
       " -26.557622160252407,\n",
       " -25.02918297850921,\n",
       " -25.04745447441173,\n",
       " -30.167375866447216,\n",
       " -30.540414236568758,\n",
       " -20.586161831132486,\n",
       " -26.06057020472384,\n",
       " -25.44054780874456,\n",
       " -7.446390603387394,\n",
       " -30.976281522662205,\n",
       " -26.53875560385147,\n",
       " -30.209600603372223,\n",
       " -26.079223274473037,\n",
       " -22.09023922790876]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_value = predict(X_train_int,X_train_initial_states_dict,state_transition_matrix)\n",
    "predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-29.20711774528513,\n",
       " -24.156945232256724,\n",
       " -24.996172370067733,\n",
       " -10.330453192090296,\n",
       " -25.740601654971673,\n",
       " -28.600083578578783,\n",
       " -28.63067591329635,\n",
       " -35.790244612337794,\n",
       " -30.704156531577638]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_value = predict(X_test_int,X_train_initial_states_dict,state_transition_matrix)\n",
    "predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
